# Logstash Pipeline Configuration
input {
  # Beats input (for Filebeat, Metricbeat, etc.)
  beats {
    port => 5044
  }
  
  # TCP input for application logs
  tcp {
    port => 5000
    type => "tcp"
  }
  
  # UDP input for syslog
  udp {
    port => 5000
    type => "syslog"
  }
  
  # HTTP input for webhook-style log shipping
  http {
    port => 8080
    type => "http"
  }
  
  # Docker logs input (if using docker logging driver)
  tcp {
    port => 5140
    type => "docker"
    codec => json_lines
  }
}

filter {
  # Parse JSON logs
  if [type] == "docker" or [fields][logtype] == "json" {
    json {
      source => "message"
    }
  }
  
  # Parse syslog
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:server} %{DATA:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:msg}" }
    }
    date {
      match => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
  
  # Parse application logs
  if [fields][app] == "shell-platform" {
    grok {
      match => {
        "message" => [
          "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:service}\] %{LOGLEVEL:level}: %{GREEDYDATA:msg}",
          "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:msg}"
        ]
      }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
  }
  
  # Add common fields
  mutate {
    add_field => { "[@metadata][index]" => "logs-%{+YYYY.MM.dd}" }
  }
  
  # Parse user agent strings
  if [user_agent] {
    useragent {
      source => "user_agent"
    }
  }
  
  # GeoIP lookup for IP addresses
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # Remove fields that are not needed
  mutate {
    remove_field => [ "agent", "ecs", "input", "log" ]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index]}"
  }
  
  # Debug output (comment out in production)
  stdout {
    codec => rubydebug
  }
}
